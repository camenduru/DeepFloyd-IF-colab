{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/DeepFloyd-IF-colab/blob/main/DeepFloyd-IF-I-M-v1.0-final.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA4ASNpIvTzd"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/shonenkov/deepfloyd-if-4-3b-generator-of-pictures modified\n",
        "\n",
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "!pip install -q xformers==0.0.16 triton==2.0.0 -U\n",
        "!pip install -q deepfloyd-if==1.0.1 \n",
        "!pip install -q git+https://github.com/openai/CLIP.git --no-deps\n",
        "# !git clone https://huggingface.co/bakedpotat/prompts\n",
        "!pip install -q -U diffusers~=0.16 transformers~=4.28 safetensors~=0.3 sentencepiece~=0.1 accelerate~=0.18 bitsandbytes~=0.38 huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_ipython().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5EncoderModel\n",
        "\n",
        "hf_token = \"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\"\n",
        "\n",
        "text_encoder = T5EncoderModel.from_pretrained(\n",
        "    \"DeepFloyd/IF-I-L-v1.0\",\n",
        "    load_in_8bit=True,\n",
        "    subfolder=\"text_encoder\",\n",
        "    device_map=\"auto\",\n",
        "    variant=\"8bit\",\n",
        "    use_auth_token=hf_token\n",
        ")\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"DeepFloyd/IF-I-L-v1.0\", \n",
        "    text_encoder=text_encoder,\n",
        "    unet=None, \n",
        "    device_map=\"auto\",\n",
        "    safety_checker=None,\n",
        "    use_auth_token=hf_token\n",
        ")\n",
        "\n",
        "prompt = 'a photograph of an astronaut riding a horse holding a sign that says \"Pixel\\'s in space\"'\n",
        "prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)\n",
        "\n",
        "import numpy as np\n",
        "prompt_embeds = prompt_embeds.cpu()\n",
        "negative_embeds = negative_embeds.cpu()\n",
        "np.save('prompt.npy', prompt_embeds)\n",
        "np.save('negative.npy', negative_embeds)\n",
        "\n",
        "get_ipython().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69qUFyBkwKs0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['FORCE_MEM_EFFICIENT_ATTN'] = \"1\"\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\n",
        "\n",
        "hf_token = \"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\"\n",
        "device = 'cuda:0'\n",
        "if_I = IFStageI('IF-I-L-v1.0', device=device, hf_token=hf_token)\n",
        "if_II = IFStageII('IF-II-L-v1.0', device=device, hf_token=hf_token)\n",
        "if_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts, t5_embs = [], []\n",
        "prompt = 'a photograph of an astronaut riding a horse holding a sign that says \"Pixel\\'s in space\"'\n",
        "t5_numpy = np.load(f'/content/prompt.npy')\n",
        "t5_numpy = t5_numpy.reshape(77, 4096)\n",
        "t5_embs.append(torch.from_numpy(t5_numpy).unsqueeze(0))\n",
        "prompts.append(prompt)\n",
        "\n",
        "t5_embs = torch.cat(t5_embs).to(device)\n",
        "t5_embs.shape\n",
        "\n",
        "# Stage-I: 64px\n",
        "\n",
        "seed = 42\n",
        "\n",
        "stageI_generations, _meta = if_I.embeddings_to_image(\n",
        "    t5_embs, seed=seed, batch_repeat=1,\n",
        "    dynamic_thresholding_p=0.95,\n",
        "    dynamic_thresholding_c=1.5,\n",
        "    guidance_scale=7.0,\n",
        "    sample_loop='ddpm',\n",
        "    sample_timestep_respacing='smart50',\n",
        "    image_size=64,\n",
        "    aspect_ratio=\"1:1\",\n",
        "    progress=True,\n",
        "    disable_watermark=True,\n",
        ")\n",
        "pil_images_I = if_I.to_images(stageI_generations, disable_watermark=True)\n",
        "if_I.show(pil_images_I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage-II: 64px --> 256 px\n",
        "\n",
        "stageII_generations, _meta = if_II.embeddings_to_image(\n",
        "    stageI_generations,\n",
        "    t5_embs, seed=seed, batch_repeat=1,\n",
        "    dynamic_thresholding_p=0.95,\n",
        "    dynamic_thresholding_c=1.0,\n",
        "    aug_level=0.25,\n",
        "    guidance_scale=4.0,\n",
        "    image_scale=4.0,\n",
        "    sample_loop='ddpm',\n",
        "    sample_timestep_respacing='50',\n",
        "    progress=True,\n",
        ")\n",
        "pil_images_II = if_II.to_images(stageII_generations, disable_watermark=True)\n",
        "if_II.show(pil_images_II)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage-III: 256px --> 1024px\n",
        "\n",
        "stageIII_generations = []\n",
        "for idx in range(len(stageII_generations)):\n",
        "    if_III_kwargs = {}\n",
        "    if_III_kwargs['prompt'] = prompts[idx:idx+1]\n",
        "    if_III_kwargs['low_res'] = stageII_generations[idx:idx+1]\n",
        "    if_III_kwargs['seed'] = seed\n",
        "    if_III_kwargs['t5_embs'] = t5_embs[idx:idx+1]\n",
        "    _stageIII_generations, _meta = if_III.embeddings_to_image(**if_III_kwargs)\n",
        "    stageIII_generations.append(_stageIII_generations)\n",
        "\n",
        "stageIII_generations = torch.cat(stageIII_generations, 0)\n",
        "pil_images_III = if_III.to_images(stageIII_generations, disable_watermark=True)\n",
        "\n",
        "for idx in range(len(prompts)):\n",
        "    pil_img, prompt = pil_images_III[idx], prompts[idx]\n",
        "    pil_img.save(f'{idx}.png')\n",
        "    if_I.show([pil_img],size=14)\n",
        "    print(prompt, '\\n'*3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
