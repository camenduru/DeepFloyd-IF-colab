{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/DeepFloyd-IF-colab/blob/main/DeepFloyd-IF-I-M-v1.0-test.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA4ASNpIvTzd"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/shonenkov/deepfloyd-if-4-3b-generator-of-pictures modified\n",
        "\n",
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "!pip install -q xformers==0.0.16 triton==2.0.0 -U\n",
        "!pip install -q deepfloyd-if==1.0.1 \n",
        "!pip install -q git+https://github.com/openai/CLIP.git --no-deps\n",
        "!git clone https://huggingface.co/bakedpotat/prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69qUFyBkwKs0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['FORCE_MEM_EFFICIENT_ATTN'] = \"1\"\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\n",
        "\n",
        "hf_token = \"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\"\n",
        "device = 'cuda:0'\n",
        "if_I = IFStageI('IF-I-L-v1.0', device=device, hf_token=hf_token)\n",
        "# if_I = IFStageI('IF-I-XL-v1.0', device=device, hf_token=hf_token)\n",
        "# if_II = IFStageII('IF-II-L-v1.0', device=device, hf_token=hf_token)\n",
        "# if_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)\n",
        "\n",
        "prompts, t5_embs = [], []\n",
        "for prompt_idx in [289, 114, 255, 38]:\n",
        "    prompt = open(f'/content/prompts/{str(prompt_idx).zfill(4)}.txt').read().strip()\n",
        "    t5_numpy = np.load(f'/content/prompts/{str(prompt_idx).zfill(4)}.npy')\n",
        "    t5_embs.append(torch.from_numpy(t5_numpy).unsqueeze(0))\n",
        "    prompts.append(prompt)\n",
        "\n",
        "t5_embs = torch.cat(t5_embs).to(device)\n",
        "t5_embs.shape\n",
        "\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stageI_generations, _meta = if_I.embeddings_to_image(\n",
        "    t5_embs, seed=seed, batch_repeat=1,\n",
        "    dynamic_thresholding_p=0.95,\n",
        "    dynamic_thresholding_c=1.5,\n",
        "    guidance_scale=7.0,\n",
        "    sample_loop='ddpm',\n",
        "    sample_timestep_respacing='smart50',\n",
        "    image_size=64,\n",
        "    aspect_ratio=\"1:1\",\n",
        "    progress=True,\n",
        ")\n",
        "pil_images_I = if_I.to_images(stageI_generations)\n",
        "if_I.show(pil_images_I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stageII_generations, _meta = if_II.embeddings_to_image(\n",
        "    stageI_generations,\n",
        "    t5_embs, seed=seed, batch_repeat=1,\n",
        "    dynamic_thresholding_p=0.95,\n",
        "    dynamic_thresholding_c=1.0,\n",
        "    aug_level=0.25,\n",
        "    guidance_scale=4.0,\n",
        "    image_scale=4.0,\n",
        "    sample_loop='ddpm',\n",
        "    sample_timestep_respacing='50',\n",
        "    progress=True,\n",
        ")\n",
        "pil_images_II = if_II.to_images(stageII_generations)\n",
        "if_II.show(pil_images_II)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stageIII_generations = []\n",
        "for idx in range(len(stageII_generations)):\n",
        "    if_III_kwargs = {}\n",
        "    if_III_kwargs['prompt'] = prompts[idx:idx+1]\n",
        "    if_III_kwargs['low_res'] = stageII_generations[idx:idx+1]\n",
        "    if_III_kwargs['seed'] = seed\n",
        "    if_III_kwargs['t5_embs'] = t5_embs[idx:idx+1]\n",
        "    _stageIII_generations, _meta = if_III.embeddings_to_image(**if_III_kwargs)\n",
        "    stageIII_generations.append(_stageIII_generations)\n",
        "\n",
        "stageIII_generations = torch.cat(stageIII_generations, 0)\n",
        "pil_images_III = if_III.to_images(stageIII_generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx in range(len(prompts)):\n",
        "    pil_img, prompt = pil_images_III[idx], prompts[idx]\n",
        "    pil_img.save(f'{idx}.png')\n",
        "    if_I.show([pil_img],size=14)\n",
        "    print(prompt, '\\n'*3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
